{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.2.0-cp37-cp37m-win_amd64.whl (459.2 MB)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Using cached h5py-2.10.0-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Using cached grpcio-1.30.0-cp37-cp37m-win_amd64.whl (2.3 MB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in c:\\users\\sb\\anaconda3\\lib\\site-packages (from tensorflow) (0.33.6)\n",
      "Collecting gast==0.3.3\n",
      "  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Using cached tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Processing c:\\users\\sb\\appdata\\local\\pip\\cache\\wheels\\3f\\e3\\ec\\8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\\termcolor-1.1.0-py3-none-any.whl\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Processing c:\\users\\sb\\appdata\\local\\pip\\cache\\wheels\\cc\\af\\1a\\498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\\absl_py-0.9.0-py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.1.0\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.4)\n",
      "Collecting scipy==1.4.1; python_version >= \"3\"\n",
      "  Using cached scipy-1.4.1-cp37-cp37m-win_amd64.whl (30.9 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Using cached tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sb\\anaconda3\\lib\\site-packages (from protobuf>=3.8.0->tensorflow) (41.4.0)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.1)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.23.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Using cached google_auth-1.18.0-py2.py3-none-any.whl (90 kB)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\sb\\anaconda3\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.9)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Using cached cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Using cached rsa-4.6-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sb\\anaconda3\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Installing collected packages: h5py, grpcio, gast, tensorflow-estimator, astunparse, termcolor, google-pasta, absl-py, keras-preprocessing, scipy, opt-einsum, cachetools, pyasn1, pyasn1-modules, rsa, google-auth, google-auth-oauthlib, markdown, tensorboard-plugin-wit, tensorboard, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.3.1\n",
      "    Uninstalling scipy-1.3.1:\n",
      "      Successfully uninstalled scipy-1.3.1\n",
      "Successfully installed absl-py-0.9.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.18.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.2.2 opt-einsum-3.2.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 rsa-4.6 scipy-1.4.1 tensorboard-2.2.2 tensorboard-plugin-wit-1.7.0 tensorflow-2.2.0 tensorflow-estimator-2.2.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution() # tensorflow v2 버전 비활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "h_size = 28\n",
    "w_size = 28\n",
    "c_size = 1\n",
    "hidden_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 이미지를 변형하여 벡터의 시퀀스로 만드는 부분입니다. x_raw는 28x28 크기의 MNIST 이미지이고, 이를 tf.split을 이용해서 28조각으로 자르면 28차원 벡터가 28개 나열되어 있는 시퀀스로 만들 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_raw = tf.compat.v1.placeholder(tf.float32, shape=[batch_size, h_size, w_size, c_size])\n",
    "# [100, 28, 28, 1] \n",
    "x_split = tf.split(x_raw, h_size, axis=1)\n",
    "# [100, 28, 28, 1] -> list of [100, 1, 28, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0부터 9까지의 label을 y로 두겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.compat.v1.placeholder(tf.float32, shape=[batch_size, 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 모델 정의에 필요한 변수들인 U, W, V 및 hidden state s를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\SB\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "U = tf.Variable(tf.compat.v1.random_normal([w_size, hidden_size], stddev=0.01))\n",
    "W = tf.Variable(tf.compat.v1.random_normal([hidden_size, hidden_size], stddev=0.01)) # always square\n",
    "V = tf.Variable(tf.compat.v1.random_normal([hidden_size, 10], stddev=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = {}\n",
    "s_init = tf.compat.v1.random_normal(shape=[batch_size, hidden_size], stddev=0.01)\n",
    "s[-1] = s_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "st=tanh(xtU+st−1W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t, x_split in enumerate(x_split):\n",
    "    x = tf.reshape(x_split, [batch_size, w_size]) # [100, 1, 28, 1] -> [100, 28]\n",
    "    s[t] = tf.nn.tanh(tf.matmul(x, U) + tf.matmul(s[t-1], W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출력, 즉 예측값은 마지막 hidden state st로부터 계산됩니다. st와 V를 곱하는데, 여기서 V는 hidden state와 출력을 연결시켜주며 출력 벡터의 크기를 맞춰주는 역할을 합니다. 마지막으로 출력을 확률값으로 변환하기 위해 softmax 함수를 적용합니다. softmax 함수는 모든 출력값을 0 ~ 1 사이로 변환하고, 출력값의 합이 1이 되도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = tf.nn.softmax(tf.matmul(s[h_size-1], V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 비용함수는 맞춰야 하는 숫자 y와 모델의 출력인 o 사이의 cross entropy로 정해집니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -tf.compat.v1.reduce_mean(tf.compat.v1.log(tf.reduce_sum(o*y, axis=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적화 알고리즘으로는 Gradient Descent를 쓰고, 학습 속도는 0.1로 정하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "trainer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow에서 실제 계산을 수행하기 위해서는 세션을 열어야 합니다. 여기서는 결과를 즉각적으로 확인하기 위해 interactive session을 열겠습니다. 세션을 연 후에는 모든 변수를 초기화시켜 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SB\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.InteractiveSession()\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "init.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리가 데이터셋으로 쓸 MNIST 데이터를 가져오는 코드입니다. 데이터가 컴퓨터에 있다면 가져오고, 만약 없다면 자동으로 인터넷에서 다운로드를 받습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-49-8bf8ae5a5303>:2: read_data_sets (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
      "WARNING:tensorflow:From C:\\Users\\SB\\Anaconda3\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:297: _maybe_download (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\SB\\Anaconda3\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:299: _extract_images (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\SB\\Anaconda3\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:304: _extract_labels (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\SB\\Anaconda3\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:112: _dense_to_one_hot (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\SB\\Anaconda3\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:328: _DataSet.__init__ (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True, reshape=False)\n",
    "trainimgs, trainlabels, testimgs, testlabels \\\n",
    " = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels \n",
    "ntrain, ntest, dim, nclasses \\\n",
    " = trainimgs.shape[0], testimgs.shape[0], trainimgs.shape[1], trainlabels.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델을 평가하기 위해 평가용 데이터셋을 따로 떼놓겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = testimgs[:batch_size]\n",
    "test_outputs = testlabels[:batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 정확도를 계산하기 위한 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(network, t):\n",
    "    \n",
    "    t_predict = tf.argmax(network, axis=1)\n",
    "    t_actual = tf.argmax(t, axis=1)\n",
    "\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(t_predict, t_actual), tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy(o, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 학습이 수행되는 부분입니다. 여기서는 간단하게 20번만 학습을 시켜보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(20):\n",
    "    for i in range(trainlabels.shape[0]//batch_size):\n",
    "        inputs = trainimgs[i*batch_size:batch_size*(1+i)]\n",
    "        outputs = trainlabels[i*batch_size:batch_size*(1+i)]\n",
    "        feed = {x_raw:inputs, y:outputs}\n",
    "        trainer.run(feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc.eval({x_raw:test_inputs, y:test_outputs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
